<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Summary &mdash; ESPnet 202209 documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> ESPnet
          </a>
              <div class="version">
                202209
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p><span class="caption-text">Tutorial:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorial.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../parallelization.html">Using Job scheduling system</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docker.html">Docker</a></li>
</ul>
<p><span class="caption-text">ESPnet2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../espnet2_tutorial.html">ESPnet2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../espnet2_tutorial.html#instruction-for-run-sh">Instruction for run.sh</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../espnet2_training_option.html">Change the configuration for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../espnet2_task.html">Task class and data input system for training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../espnet2_distributed.html">Distributed training</a></li>
</ul>
<p><span class="caption-text">Notebook:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/asr_cli.html">Speech Recognition (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/asr_library.html">Speech Recognition (Library)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_asr_realtime_demo.html">ESPnet2-ASR realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_asr_transfer_learning_demo.html"><strong>Use transfer learning for ASR in ESPnet2</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_asr_transfer_learning_demo.html#Abstract">Abstract</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_asr_transfer_learning_demo.html#ESPnet-installation-(about-10-minutes-in-total)">ESPnet installation (about 10 minutes in total)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_asr_transfer_learning_demo.html#mini_an4-recipe-as-a-transfer-learning-example">mini_an4 recipe as a transfer learning example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html">CMU 11751/18781 Fall 2022: ESPnet Tutorial2 (New task)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html#Install-ESPnet-(Almost-same-procedure-as-your-first-tutorial)">Install ESPnet (Almost same procedure as your first tutorial)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_new_task_tutorial_CMU_11751_18781_Fall2022.html#What-we-provide-you-and-what-you-need-to-proceed">What we provide you and what you need to proceed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html">CMU 11751/18781 Fall 2022: ESPnet Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Install-ESPnet">Install ESPnet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Run-an-existing-recipe">Run an existing recipe</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Make-a-new-recipe">Make a new recipe</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_recipe_tutorial_CMU_11751_18781_Fall2022.html#Additional-resources">Additional resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_streaming_asr_demo.html">ESPnet2 real streaming Transformer demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_tts_realtime_demo.html">ESPnet2-TTS realtime demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_tutorial_2021_CMU_11751_18781.html">CMU 11751/18781 2021: ESPnet Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Run-an-inference-example">Run an inference example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Full-installation">Full installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet2_tutorial_2021_CMU_11751_18781.html#Run-a-recipe-example">Run a recipe example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet_se_demonstration_for_waspaa_2021.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet_se_demonstration_for_waspaa_2021.html#Contents">Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet_se_demonstration_for_waspaa_2021.html#(1)-Tutorials-on-the-Basic-Usage">(1) Tutorials on the Basic Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/espnet_se_demonstration_for_waspaa_2021.html#(2)-Tutorials-on-Contributing-to-ESPNet-SE-Project">(2) Tutorials on Contributing to ESPNet-SE Project</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/onnx_conversion_demo.html">espnet_onnx demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/onnx_conversion_demo.html#Install-Dependency">Install Dependency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/onnx_conversion_demo.html#Export-your-model">Export your model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/onnx_conversion_demo.html#Inference-with-onnx">Inference with onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/onnx_conversion_demo.html#Using-streaming-model">Using streaming model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/pretrained.html">Pretrained Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/se_demo.html">ESPnet Speech Enhancement Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/st_demo.html">ESPnet Speech Translation Demonstration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/tts_cli.html">Text-to-Speech (Recipe)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebook/tts_realtime_demo.html">ESPnet real time E2E-TTS demonstration</a></li>
</ul>
<p><span class="caption-text">Package Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.scheduler.html">espnet.scheduler package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.nets.html">espnet.nets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.asr.html">espnet.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.transform.html">espnet.transform package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.lm.html">espnet.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.st.html">espnet.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.utils.html">espnet.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.optimizer.html">espnet.optimizer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.tts.html">espnet.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.vc.html">espnet.vc package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.bin.html">espnet.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.mt.html">espnet.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet.distributed.html">espnet.distributed package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.fileio.html">espnet2.fileio package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.asr.html">espnet2.asr package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.gan_tts.html">espnet2.gan_tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.lm.html">espnet2.lm package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.st.html">espnet2.st package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.utils.html">espnet2.utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.layers.html">espnet2.layers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.samplers.html">espnet2.samplers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.optimizers.html">espnet2.optimizers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.fst.html">espnet2.fst package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.text.html">espnet2.text package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.schedulers.html">espnet2.schedulers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.tts.html">espnet2.tts package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.torch_utils.html">espnet2.torch_utils package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.bin.html">espnet2.bin package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.iterators.html">espnet2.iterators package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.main_funcs.html">espnet2.main_funcs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.hubert.html">espnet2.hubert package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.tasks.html">espnet2.tasks package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.slu.html">espnet2.slu package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.train.html">espnet2.train package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.asr_transducer.html">espnet2.asr_transducer package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.enh.html">espnet2.enh package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.diar.html">espnet2.diar package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.mt.html">espnet2.mt package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../_gen/espnet2.svs.html">espnet2.svs package</a></li>
</ul>
<p><span class="caption-text">Tool Reference:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../apis/espnet_bin.html">core tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../apis/espnet2_bin.html">core tools (espnet2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../apis/utils_py.html">python utility tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../apis/utils_sh.html">bash utility tools</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">ESPnet</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Summary</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/paper/espnet-se++/paper.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<hr class="docutils" />
<p>title: ‘Software Design and User Interface of ESPnet-SE++: Speech Enhancement for Robust Speech Processing’
tags:</p>
<ul class="simple">
<li><p>Python</p></li>
<li><p>ESPnet</p></li>
<li><p>speech processing</p></li>
<li><p>speech enhancement
authors:</p></li>
<li><p>name: Yen-Ju Lu
orcid: 0000-0001-8400-4188
equal-contrib: true
affiliation: 1</p></li>
<li><p>name: Xuankai Chang
orcid: 0000-0002-5221-5412
equal-contrib: true
affiliation: 2</p></li>
<li><p>name: Chenda Li
orcid: 0000-0003-0299-9914
affiliation: 3</p></li>
<li><p>name: Wangyou Zhang
orcid: 0000-0003-4500-3515
affiliation: 3</p></li>
<li><p>name: Samuele Cornell
orcid: 0000-0002-5358-1844
affiliation: “2, 4”</p></li>
<li><p>name: Zhaoheng Ni
affiliation: 5</p></li>
<li><p>name: Yoshiki Masuyama
affiliation: “2, 6”</p></li>
<li><p>name: Brian Yan
affiliation: 2</p></li>
<li><p>name: Robin Scheibler
orcid: 0000-0002-5205-8365
affiliation: 7</p></li>
<li><p>name: Zhong-Qiu Wang
orcid: 0000-0002-4204-9430
affiliation: 2</p></li>
<li><p>name: Yu Tsao
orcid: 0000-0001-6956-0418
affiliation: 8</p></li>
<li><p>name: Yanmin Qian
orcid: 0000-0002-0314-3790
affiliation: 3</p></li>
<li><p>name: Shinji Watanabe
corresponding: true
orcid: 0000-0002-5970-8631
affiliation: 2
affiliations:</p></li>
<li><p>name: Johns Hopkins University, USA
index: 1</p></li>
<li><p>name: Carnegie Mellon University, USA
index: 2</p></li>
<li><p>name: Shanghai Jiao Tong University, Shanghai
index: 3</p></li>
<li><p>name: Universita` Politecnica delle Marche, Italy
index: 4</p></li>
<li><p>name: Meta AI, USA
index: 5</p></li>
<li><p>name: Tokyo Metropolitan University, Japan
index: 6</p></li>
<li><p>name: LINE Corporation, Japan
index: 7</p></li>
<li><p>name: Academia Sinica, Taipei
index: 8
date: 22 August 2022
bibliography: paper.bib</p></li>
</ul>
<hr class="docutils" />
<p><img alt="../../_images/espnet-SE++.png" src="../../_images/espnet-SE++.png" /></p>
<section id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h1>
<p>This paper presents the software design and user interface of ESPnet-SE++, a new speech separation and enhancement (SSE) module of the ESPnet toolkit.
ESPnet-SE++ expands significantly the functionality of ESPnet-SE [&#64;Li:2021] with several new models, loss functions and recipes [&#64;Lu:2022]. Crucially, it features a new, redesigned interface, which allows for a flexible combination of SSE front-ends with many downstream tasks, including automatic speech recognition (ASR), speaker diarization (SD), speech translation (ST), and spoken language understanding (SLU).</p>
</section>
<section id="statement-of-need">
<h1>Statement of need<a class="headerlink" href="#statement-of-need" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://github.com/espnet/espnet">ESPnet</a> is an open-source toolkit for speech processing, including several ASR, text-to-speech (TTS) [&#64;Hayashi:2020], ST [&#64;Inaguma:2020], machine translation (MT), SLU [&#64;Arora:2022], and SSE recipes [&#64;Watanabe:2018]. Compared with other open-source SSE toolkits, such as Nussl [&#64;Manilow:2018], Onssen [&#64;Ni:2019], Asteroid [&#64;Pariente:2020], and SpeechBrain [&#64;Ravanelli:2021], the modularized design in ESPnet-SE++ allows for the joint combination of SSE modules with other tasks, such as SLU and ASR. Currently, ESPnet-SE++ supports 20 SSE recipes with 24 different enhancement/separation models.</p>
<!-- helps implement the joint-task modules of SSE. --></section>
<section id="espnet-se-recipes-and-software-structure">
<h1>ESPnet-SE++ Recipes and Software Structure<a class="headerlink" href="#espnet-se-recipes-and-software-structure" title="Permalink to this headline">¶</a></h1>
<p>ESPnet-SE++ is part of the ESPnet2 framework, and includes the recipes under the <code class="docutils literal notranslate"><span class="pre">egs2</span></code> folder and the modularized SSE and joint-task scripts and models under the <code class="docutils literal notranslate"><span class="pre">espnet2</span></code> folder. This section describes how these recipes and the ESPNet-SE++ modular codebase are structured.</p>
<section id="espnet-se-recipes-for-sse-and-joint-task">
<h2>ESPNet-SE++ Recipes for SSE and Joint-Task<a class="headerlink" href="#espnet-se-recipes-for-sse-and-joint-task" title="Permalink to this headline">¶</a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>espnet/
└── egs2/
    ├── chime4/
    │   ├── enh1/
    │   ├── enh_asr1/
    │   └── asr1/
    ├── l3das22/
    │   └── enh1/
    |   │   ├── conf/
    |   │   ├── local/
    |   |   │   ├── data.sh
    |   |   │   ├── metric.sh
    │   |   │   └── ...
    |   │   ├── enh.sh -&gt; ../../TEMPLATE/enh1/enh.sh
    |   │   ├── run.sh
    |   │   └── ...
    ├── lt_slurp_spatialized/
    │   └── enh1/
    ├── slurp_spatialized/
    │   ├── enh_asr1/
    |   │   ├── enh_asr.sh -&gt; ../../TEMPLATE/enh_asr1/enh_asr.sh
    |   │   ├── run.sh
    |   │   └── ...
    │   └── asr1/
    ├── ...
    └── TEMPLATE/
        ├── enh1/
        │   └── enh.sh
        ├── enh_asr1/
        │   └── enh_asr.sh
        ├── enh_diar1/
        │   └── enh_diar.sh
        ├── enh_st1/
        │   └── enh_st.sh
        └── ...
</pre></div>
</div>
<p>For each task, ESPnet-SE++, following ESPnet2 style, provides common scripts which are carefully designed to work out-of-the-box with a wide variety of corpora. Under the <code class="docutils literal notranslate"><span class="pre">TEMPLATE</span></code> folder, the common scripts <code class="docutils literal notranslate"><span class="pre">enh1/enh.sh</span></code> and <code class="docutils literal notranslate"><span class="pre">enh_asr1/enh_asr.sh</span></code> are shared for all the SSE and joint-task recipes. Symbolic links to the shared scripts are created under each <code class="docutils literal notranslate"><span class="pre">enh1</span></code> and <code class="docutils literal notranslate"><span class="pre">enh_asr1</span></code> corpus folders and called by their <code class="docutils literal notranslate"><span class="pre">run.sh</span></code>.</p>
<section id="common-scripts">
<h3>Common Scripts<a class="headerlink" href="#common-scripts" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">enh.sh</span></code> contains 13 stages, and the details for the scripts can be found in <a class="reference external" href="https://github.com/espnet/espnet/blob/master/egs2/TEMPLATE/enh1/README.md">TEMPLATE/enh1/README.md</a>.</p>
<ul class="simple">
<li><p>stage 1 to stage 4: data preparation stages</p>
<ul>
<li><p>stage 1: Call the local/data.sh script from the recipe to generate Kaldi-style data [&#64;Povey:2011] directories for each subset.</p></li>
<li><p>stage 2: Optional offline augmentation of input dataset (e.g. speed perturbation).</p></li>
<li><p>stage 3: Create a temporary data dump folder, segment audio files, and change the audio format and sampling rate if needed. This step normalizes the general format of audio files, and thus enables to combine  different corpora at training or inference time.</p></li>
<li><p>stage 4: Possibly remove too short and too long utterances</p></li>
</ul>
</li>
<li><p>stage 5 to stage 6: SSE training steps</p>
<ul>
<li><p>stage 5: Collect dataset statistics which can be used to sort examples for dataloading purposes or for normalization</p></li>
<li><p>stage 6: SSE task training</p></li>
</ul>
</li>
<li><p>stage 7 to stage 8: Evaluation stages: inferencing and storing the enhanced audios (stage 7), and scoring (stage 8)</p></li>
<li><p>stage 9 to stage 10: Evaluation stages for speech recognition or understanding: decoding with a pretrained ASR/SLU model (stage 9) and scoring with a pretrained ASR model
(stage 10)</p></li>
<li><p>stage 11 to stage 13: model uploading steps, upload the trained model to Zenodo or Hugging Face through these three final steps.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">enh_asr.sh</span></code> contains 17 stages and <code class="docutils literal notranslate"><span class="pre">enh_diar.sh</span></code> and <code class="docutils literal notranslate"><span class="pre">enh_st.sh</span></code> are similar to it:</p>
<ul class="simple">
<li><p>stage 1 to stage 5: data preparation stages</p></li>
<li><p>stage 6 to stage 9: language model training steps</p></li>
<li><p>stage 10 to stage 11: joint-task training steps</p></li>
<li><p>stage 12 to stage 13: Inference stages: Decoding (stage 12), and enhancing (stage 13)</p></li>
<li><p>stage 14 to stage 15:  Scoring recognition results (stage 14) and SSE results (stage 15)</p></li>
<li><p>stage 16 to stage 17: model uploading steps, upload the trained model through the final steps.</p></li>
</ul>
<!-- ## Software Structure for SSE and Joint-task --></section>
<section id="training-configuration">
<h3>Training Configuration<a class="headerlink" href="#training-configuration" title="Permalink to this headline">¶</a></h3>
<p>After the data preparation stages of <code class="docutils literal notranslate"><span class="pre">enh.sh</span></code>  and <code class="docutils literal notranslate"><span class="pre">enh_asr.sh</span></code>, the training and inference stages call the SSE and joint-task interfaces under <code class="docutils literal notranslate"><span class="pre">espnet2</span></code> according to the training configuration in the <code class="docutils literal notranslate"><span class="pre">conf</span></code> folder.</p>
<section id="sse-task-training-configuration">
<h4>SSE Task Training Configuration<a class="headerlink" href="#sse-task-training-configuration" title="Permalink to this headline">¶</a></h4>
<p>An example of an enhancement task for the CHiME-4 <code class="docutils literal notranslate"><span class="pre">enh1</span></code>  recipe is configured as <a class="reference external" href="https://github.com/espnet/espnet/blob/master/egs2/chime4/enh1/conf/tuning/train_enh_dprnn_tasnet.yaml"><code class="docutils literal notranslate"><span class="pre">conf/tuning/train_enh_dprnn_tasnet.yaml</span></code></a>. Part of this configuration is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">encoder</span><span class="p">:</span> <span class="n">conv</span>
<span class="n">encoder_conf</span><span class="p">:</span>
    <span class="n">channel</span><span class="p">:</span> <span class="mi">64</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">2</span>
    <span class="n">stride</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">decoder</span><span class="p">:</span> <span class="n">conv</span>
<span class="n">decoder_conf</span><span class="p">:</span>
    <span class="n">channel</span><span class="p">:</span> <span class="mi">64</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">2</span>
    <span class="n">stride</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">separator</span><span class="p">:</span> <span class="n">dprnn</span>
<span class="n">separator_conf</span><span class="p">:</span>
    <span class="n">num_spk</span><span class="p">:</span> <span class="mi">1</span>
    <span class="n">layer</span><span class="p">:</span> <span class="mi">6</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="n">lstm</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="kc">True</span>  <span class="c1"># this is for the inter-block rnn</span>
    <span class="n">nonlinear</span><span class="p">:</span> <span class="n">relu</span>
    <span class="n">unit</span><span class="p">:</span> <span class="mi">128</span>
    <span class="n">segment_size</span><span class="p">:</span> <span class="mi">250</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="mf">0.1</span>
    <span class="n">nonlinear</span><span class="p">:</span> <span class="n">relu</span>

<span class="n">criterions</span><span class="p">:</span> 
  <span class="c1"># The first criterion</span>
  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">si_snr</span> 
    <span class="n">conf</span><span class="p">:</span>
      <span class="n">eps</span><span class="p">:</span> <span class="mf">1.0e-7</span>
    <span class="c1"># the wrapper for the current criterion</span>
    <span class="c1"># for single-talker case, we simplely use fixed_order wrapper</span>
    <span class="n">wrapper</span><span class="p">:</span> <span class="n">fixed_order</span>
    <span class="n">wrapper_conf</span><span class="p">:</span>
      <span class="n">weight</span><span class="p">:</span> <span class="mf">1.0</span>
</pre></div>
</div>
<p>The training configuration describes a single-channel speech enhancement task with a DPRNN [&#64;Luo:2020] model based on a convolutional encoder-masker-decoder framework, popularized by TasNet [&#64;Luo:2018] and Conv-Tasnet [&#64;Luo:2019]. In addition, loss-related modules, wrapper and criterion, are specified. The remaining configurations describe the training strategy and the optimizer configuration.</p>
<!-- the network structure of all the submodules of the SSE module,  --></section>
<section id="joint-task-training-configuration">
<h4>Joint-Task Training Configuration<a class="headerlink" href="#joint-task-training-configuration" title="Permalink to this headline">¶</a></h4>
<p>An example of joint-task training configuration is the CHiME-4 <code class="docutils literal notranslate"><span class="pre">enh_asr1</span></code> recipe, configured as <a class="reference external" href="https://github.com/espnet/espnet/blob/master/egs2/chime4/enh_asr1/conf/tuning/train_enh_asr_convtasnet_si_snr_fbank_transformer_lr2e-3_accum2_warmup20k_specaug.yaml"><code class="docutils literal notranslate"><span class="pre">conf/tuning/train_enh_asr_convtasnet.yaml</span></code></a>. This joint-task includes a front-end enhancmenet model and a back-end ASR model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># network architecture</span>
<span class="n">enh_encoder</span><span class="p">:</span> <span class="n">conv</span>
<span class="n">enh_encoder_conf</span><span class="p">:</span>
    <span class="n">channel</span><span class="p">:</span> <span class="mi">256</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">40</span>
    <span class="n">stride</span><span class="p">:</span> <span class="mi">20</span>
<span class="n">enh_decoder</span><span class="p">:</span> <span class="n">conv</span>
<span class="n">enh_decoder_conf</span><span class="p">:</span>
    <span class="n">channel</span><span class="p">:</span> <span class="mi">256</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="mi">40</span>
    <span class="n">stride</span><span class="p">:</span> <span class="mi">20</span>
<span class="n">enh_separator</span><span class="p">:</span> <span class="n">tcn</span>
<span class="n">enh_separator_conf</span><span class="p">:</span>
    <span class="n">num_spk</span><span class="p">:</span> <span class="mi">1</span>
    <span class="n">layer</span><span class="p">:</span> <span class="mi">4</span>
    <span class="n">stack</span><span class="p">:</span> <span class="mi">2</span>
    <span class="n">bottleneck_dim</span><span class="p">:</span> <span class="mi">256</span>
    <span class="n">hidden_dim</span><span class="p">:</span> <span class="mi">512</span>
    <span class="n">kernel</span><span class="p">:</span> <span class="mi">3</span>
    <span class="n">causal</span><span class="p">:</span> <span class="kc">False</span>
    <span class="n">norm_type</span><span class="p">:</span> <span class="s2">&quot;gLN&quot;</span>
    <span class="n">nonlinear</span><span class="p">:</span> <span class="n">relu</span>
<span class="n">enh_criterions</span><span class="p">:</span>
  <span class="c1"># The first criterion</span>
  <span class="o">-</span> <span class="n">name</span><span class="p">:</span> <span class="n">si_snr</span>
    <span class="n">conf</span><span class="p">:</span>
        <span class="n">eps</span><span class="p">:</span> <span class="mf">1.0e-7</span>
    <span class="c1"># the wrapper for the current criterion</span>
    <span class="c1"># for single-talker case, we simplely use fixed_order wrapper</span>
    <span class="n">wrapper</span><span class="p">:</span> <span class="n">fixed_order</span>
    <span class="n">wrapper_conf</span><span class="p">:</span>
        <span class="n">weight</span><span class="p">:</span> <span class="mf">1.0</span>

<span class="n">frontend</span><span class="p">:</span> <span class="n">default</span>
<span class="n">frontend_conf</span><span class="p">:</span>
    <span class="n">fs</span><span class="p">:</span> <span class="mi">16000</span>
    <span class="n">n_fft</span><span class="p">:</span> <span class="mi">512</span>
    <span class="n">win_length</span><span class="p">:</span> <span class="mi">400</span>
    <span class="n">hop_length</span><span class="p">:</span> <span class="mi">160</span>
    <span class="n">frontend_conf</span><span class="p">:</span> <span class="n">null</span>
    <span class="n">apply_stft</span><span class="p">:</span> <span class="kc">True</span>

<span class="c1"># encoder related</span>
<span class="n">asr_encoder</span><span class="p">:</span> <span class="n">transformer</span>
<span class="n">asr_encoder_conf</span><span class="p">:</span>
    <span class="n">output_size</span><span class="p">:</span> <span class="mi">256</span>
    <span class="n">attention_heads</span><span class="p">:</span> <span class="mi">4</span>
    <span class="n">linear_units</span><span class="p">:</span> <span class="mi">2048</span>
    <span class="n">num_blocks</span><span class="p">:</span> <span class="mi">12</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="mf">0.1</span>
    <span class="n">attention_dropout_rate</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="n">input_layer</span><span class="p">:</span> <span class="n">conv2d</span>
    <span class="n">normalize_before</span><span class="p">:</span> <span class="n">true</span>

<span class="c1"># decoder related</span>
<span class="n">asr_decoder</span><span class="p">:</span> <span class="n">transformer</span>
<span class="n">asr_decoder_conf</span><span class="p">:</span>
    <span class="n">input_layer</span><span class="p">:</span> <span class="n">embed</span>
    <span class="n">attention_heads</span><span class="p">:</span> <span class="mi">4</span>
    <span class="n">linear_units</span><span class="p">:</span> <span class="mi">2048</span>
    <span class="n">num_blocks</span><span class="p">:</span> <span class="mi">6</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="mf">0.1</span>
    <span class="n">positional_dropout_rate</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="n">self_attention_dropout_rate</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="n">src_attention_dropout_rate</span><span class="p">:</span> <span class="mf">0.0</span>

<span class="n">asr_model_conf</span><span class="p">:</span>
    <span class="n">ctc_weight</span><span class="p">:</span> <span class="mf">0.3</span>
    <span class="n">lsm_weight</span><span class="p">:</span> <span class="mf">0.1</span>
    <span class="n">length_normalized_loss</span><span class="p">:</span> <span class="n">false</span>
    <span class="n">extract_feats_in_collect_stats</span><span class="p">:</span> <span class="n">false</span>

<span class="n">model_conf</span><span class="p">:</span>
    <span class="n">bypass_enh_prob</span><span class="p">:</span> <span class="mf">0.0</span>
</pre></div>
</div>
<p>For the configuration of the front-end SSE model, the joint task configuration describes a network structure identical to that of a single SSE task, including encoder, decoder, separator, wrapper, and criterion. On the other hand, the back-end ASR model also has configurations for its encoder and decoder. To avoid name confusion, the prefix <code class="docutils literal notranslate"><span class="pre">enh_</span></code> is added to the submodules of SSE and the prefix <code class="docutils literal notranslate"><span class="pre">asr_</span></code> is added to the submodules of ASR.</p>
<!-- Similar to the SSE configuration, the joint-task configuration describes the network architecture for the SSE and ASR modules.  --><p>The following section describes the interfaces, control classes, and modules of the SSE and joint-task models.</p>
</section>
</section>
</section>
<section id="espnet-se-software-structure-for-sse-task">
<h2>ESPNet-SE++ Software Structure for SSE Task<a class="headerlink" href="#espnet-se-software-structure-for-sse-task" title="Permalink to this headline">¶</a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>espnet/
└── espnet2/
    ├── bin/
    │   ├── enh_train.py
    │   ├── enh_inference.py
    │   ├── enh_scoring.py
    │   └── ...
    ├── enh/
    │   ├── decoder/
    │   ├── encoder/
    │   ├── layers/
    │   ├── loss/
    │   │	├── criterions
    │   │	└── wrappers
    │   ├── separator/
    │   ├── espnet_model.py
    │   └── ...
    ├── tasks/
    │   ├── abs_task.py
    │   ├── enh.py
    │   └── ...
    ├── train/
    │   ├── abs_espnet_model.py
    │   ├── trainer.py
    │   └── ...
    └── ...
</pre></div>
</div>
<section id="unified-modeling-language-diagram-for-espnet-se-enhancement-only-task">
<h3>Unified Modeling Language Diagram for ESPNet-SE++ Enhancement-Only Task<a class="headerlink" href="#unified-modeling-language-diagram-for-espnet-se-enhancement-only-task" title="Permalink to this headline">¶</a></h3>
<p>The code flow of ESPNet-SE++ for SSE task training and inference can be visualized in the following diagram.</p>
<p><img alt="../../_images/UML_SSE.png" src="../../_images/UML_SSE.png" /></p>
</section>
<section id="sse-executable-code-bin">
<h3>SSE Executable Code <code class="docutils literal notranslate"><span class="pre">bin/</span></code><a class="headerlink" href="#sse-executable-code-bin" title="Permalink to this headline">¶</a></h3>
<p>The design of the SSE executable code follows the same high level design and interface of the others ESPNet tasks (e.g. ASR, SLU et cetera). In addition, ESPnet-SE++ has its own scoring method for calculating several popular objective scores for the enhanced/separared speech such as SI-SDR [&#64;le:2019], STOI [&#64;Taal:2011], SDR and PESQ [&#64;Rix:2001].</p>
<section id="bin-enh-train-py">
<h4>bin.enh_train.py<a class="headerlink" href="#bin-enh-train-py" title="Permalink to this headline">¶</a></h4>
<p>As the main interface for the SSE training stage of <code class="docutils literal notranslate"><span class="pre">enh.sh</span></code>, <code class="docutils literal notranslate"><span class="pre">enh_train.py</span></code> takes the training parameters and model configurations from the arguments and calls</p>
<!--  scripts and calls  --><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">EnhancementTask</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> 
</pre></div>
</div>
<p>to build an SSE object <code class="docutils literal notranslate"><span class="pre">ESPnetEnhancementModel</span></code>, which is used to train the SSE model according to the model configuration..</p>
<p>The training procedure is controlled by the general training class <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> under <code class="docutils literal notranslate"><span class="pre">train.trainer</span></code> . After training, <code class="docutils literal notranslate"><span class="pre">enh_train.py</span></code> saves the SSE model checkpoint and configuration.</p>
</section>
<section id="bin-enh-inference-py">
<h4>bin.enh_inference.py<a class="headerlink" href="#bin-enh-inference-py" title="Permalink to this headline">¶</a></h4>
<p>As the main interface for the SSE inferencing stage of <code class="docutils literal notranslate"><span class="pre">enh.sh</span></code>, <code class="docutils literal notranslate"><span class="pre">enh_inference.py</span></code> has</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SeparateSpeech</span>
</pre></div>
</div>
<p>which builds an SSE object <code class="docutils literal notranslate"><span class="pre">ESPnetEnhancementModel</span></code> through the <code class="docutils literal notranslate"><span class="pre">EnhancementTask.build_model</span></code> class method based on a pair of configuration and a pre-trained SSE model.</p>
<p>Calling the <code class="docutils literal notranslate"><span class="pre">SeparateSpeech</span></code> object with an unprocessed audio returns a list of separated speech, where the length is the same as the number of speakers. The audio is enhanced by calling the <code class="docutils literal notranslate"><span class="pre">encoder</span></code>,  <code class="docutils literal notranslate"><span class="pre">separator</span></code>, and <code class="docutils literal notranslate"><span class="pre">decoder</span></code> classes in the SSE model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">inference</span></code> function in <code class="docutils literal notranslate"><span class="pre">enh_inference.py</span></code> contains the full inference process, which builds and calls a <code class="docutils literal notranslate"><span class="pre">SeparateSpeech</span></code> object with the data-iterator for testing and validation data.</p>
</section>
<section id="bin-enh-scoring-py">
<h4>bin.enh_scoring.py<a class="headerlink" href="#bin-enh-scoring-py" title="Permalink to this headline">¶</a></h4>
<p>The SSE scoring functions currently support calculating multiple objective scores, including STOI, ESTOI, SI-SNR, SDR, SIR, and SAR scores. These scores are calculated based on the reference signal and processed speech pairs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scoring</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span><span class="o">...</span><span class="p">,</span><span class="n">ref_scp</span><span class="p">,</span> <span class="n">inf_scp</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>The scoring function also provides a summary of the scores throurough the test dataset for the SSE scoring stage of <code class="docutils literal notranslate"><span class="pre">enh.sh</span></code>.</p>
<!-- calculates and writes scores for each utterance pair in the `ref_scp` and `inf_scp` to the output directory.  --></section>
</section>
<section id="sse-control-class-tasks">
<h3>SSE Control Class <code class="docutils literal notranslate"><span class="pre">tasks/</span></code><a class="headerlink" href="#sse-control-class-tasks" title="Permalink to this headline">¶</a></h3>
<section id="tasks-enh-py">
<h4>tasks.enh.py<a class="headerlink" href="#tasks-enh-py" title="Permalink to this headline">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EnhancementTask</span><span class="p">(</span><span class="n">AbsTask</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">EnhancementTask</span></code> is a control class which is designed for SSE task, containing class methods for building and training an SSE model, including <code class="docutils literal notranslate"><span class="pre">preprocessor</span></code>, <code class="docutils literal notranslate"><span class="pre">data_loader</span></code>, and <code class="docutils literal notranslate"><span class="pre">collate_fn</span></code>. Class method  <code class="docutils literal notranslate"><span class="pre">build_model</span></code> creates and returns an SSE object from</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">enh</span><span class="o">.</span><span class="n">espnet_model</span><span class="o">.</span><span class="n">ESPnetEnhancementModel</span>
</pre></div>
</div>
</section>
</section>
<section id="sse-modules-enh">
<h3>SSE Modules <code class="docutils literal notranslate"><span class="pre">enh/</span></code><a class="headerlink" href="#sse-modules-enh" title="Permalink to this headline">¶</a></h3>
<section id="enh-espnet-model-py">
<h4>enh.espnet_model.py<a class="headerlink" href="#enh-espnet-model-py" title="Permalink to this headline">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ESPnetEnhancementModel</span><span class="p">(</span><span class="n">AbsESPnetModel</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">ESPnetEnhancementModel</span></code> is the base class for any ESPnet-SE++ SSE model. It consists of <code class="docutils literal notranslate"><span class="pre">encoder</span></code>, <code class="docutils literal notranslate"><span class="pre">decoder</span></code>, <code class="docutils literal notranslate"><span class="pre">separator</span></code>, <code class="docutils literal notranslate"><span class="pre">criterion</span></code>, and <code class="docutils literal notranslate"><span class="pre">wrapper</span></code>. Since the <code class="docutils literal notranslate"><span class="pre">ESPnetEnhancementModel</span></code> inherits the same abstract base class <code class="docutils literal notranslate"><span class="pre">AbsESPnetModel</span></code>, it is well-aligned with other tasks such as ASR, TTS, ST, and SLU, bringing the benefits of cross-tasks combination. The forward functions of the class include three diffeent interfaces where the first one</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="k">def</span>  <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">speech_mix</span><span class="p">,</span> <span class="n">speech_ref</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">forward</span></code> follows the general design in the ESPnet single-task modules, which processes speech and only returns losses for the trainer to update the model. However, enhanced speech is necessary for joint-task training. The forward function is further divided into two sub-functions for more flexible combinations:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="k">def</span>  <span class="nf">forward_enhance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">speech_mix</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
 <span class="k">def</span>  <span class="nf">forward_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">speech_pre</span><span class="p">,</span> <span class="n">speech_ref</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">forward_enhance</span></code> function returns the enhanced speech, and the <code class="docutils literal notranslate"><span class="pre">forward_loss</span></code> function returns the loss. The joint-training methods take the enhanced speech as the input for the downstream task and the SSE loss as a part of the joint-training loss.</p>
</section>
<section id="encoder-separator-and-decoder">
<h4>encoder, separator, and decoder<a class="headerlink" href="#encoder-separator-and-decoder" title="Permalink to this headline">¶</a></h4>
<p>The SSE model contains three submodules: encoder, separator, and decoder. Encoder and decoder options include short-time Fourier transform (STFT) and inverse STFT (iSTFT) for the time-frequency (TF) domain models, and convolutional layers and transposed convolutional layers for time domain models.</p>
<p>The separator is a sequence mapping neural network, which takes the input from the encoder and generates separated output features. The number of separated features is the same as the number of the speech sources, and for most SSE models, separator only has one output feature. Finally, the decoder transforms the features into the target audios.</p>
<p>This modular design allows for the exploration of many different architectural variations with less complicated code. The same separator can be used with different encoders/decoders (e.g., SSL/Freq domain)</p>
</section>
<section id="criterion-and-wrapper">
<h4>criterion and wrapper<a class="headerlink" href="#criterion-and-wrapper" title="Permalink to this headline">¶</a></h4>
<p>The criterion is an implementation of loss functions, including time domain criteria and TF domain criteria. The time domain criteria take the estimated audio and target audio as input, and the TF domain criteria take the estimated feature and the encoded features of target audio as input. The output of the criterion is a scalar loss value.</p>
<p>Instead of calculating the loss by directly passing all the separated features and references into the criterion, <code class="docutils literal notranslate"><span class="pre">ESPnetEnhancementModel</span></code> calculates the loss through the wrapper combined with a criterion. For example, the Permutation invariant training (PIT) [&#64;Yu:2017] algorithm is designed as <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">PITSolver(AbsLossWrapper)</span></code> with the procedure to find the best permutation. This modular design allows researchers to design custom wrapper classes for implementing complicated training objectives conveniently.</p>
</section>
</section>
</section>
<section id="espnet-se-software-structure-for-joint-task">
<h2>ESPNet-SE++ Software Structure for Joint-Task<a class="headerlink" href="#espnet-se-software-structure-for-joint-task" title="Permalink to this headline">¶</a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>espnet/
└── espnet2/
    ├── bin/
    │   ├── asr_inference.py
    │   ├── diar_inference.py
    │   ├── enh_s2t_train.py
    │   ├── st_inference.py
    │   └── ...
    ├── enh/
    │   ├── espnet_enh_s2t_model.py
    │   └── ...
    ├── tasks/
    │   ├── enh_s2t.py
    │   └── ...
    └── ...
    
</pre></div>
</div>
<p>The design of the joint-task code follows the same design as SSE and other tasks, where the <code class="docutils literal notranslate"><span class="pre">enh_s2t</span></code> stands for the joint-task of SSE and a downstream speech-to-text (s2t) task, including automatic speech recognition (ASR), spoken language understanding (SLU), speech translation (ST), and speaker diarization (SD) tasks.</p>
<section id="unified-modeling-language-diagram-for-espnet-se-joint-task">
<h3>Unified Modeling Language Diagram for ESPNet-SE++ Joint-Task<a class="headerlink" href="#unified-modeling-language-diagram-for-espnet-se-joint-task" title="Permalink to this headline">¶</a></h3>
<p>The code flow of ESPNet-SE++ for joint-task training and inference can be visualized as the following diagram.</p>
<p><img alt="../../_images/UML_Joint.png" src="../../_images/UML_Joint.png" /></p>
</section>
<section id="joint-task-executable-code-bin">
<h3>Joint-Task Executable Code <code class="docutils literal notranslate"><span class="pre">bin/</span></code><a class="headerlink" href="#joint-task-executable-code-bin" title="Permalink to this headline">¶</a></h3>
<p>Executable scripts for joint-task include training and inferencing code. Currently, joint-task does not have their task-specific methods.</p>
<section id="bin-enh-s2t-train-py">
<h4>bin.enh_s2t_train.py<a class="headerlink" href="#bin-enh-s2t-train-py" title="Permalink to this headline">¶</a></h4>
<p>Similar to the interface of SSE training code <code class="docutils literal notranslate"><span class="pre">enh_train.py</span></code>, <code class="docutils literal notranslate"><span class="pre">enh_s2t_train.py</span></code> takes the training parameters and modular parameters from the training stage of the joint-task scripts, and calls</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tasks</span><span class="o">.</span><span class="n">enh_s2t</span><span class="o">.</span><span class="n">EnhS2TTask</span><span class="o">.</span><span class="n">main</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> 
</pre></div>
</div>
<p>to build a joint-task object for training the joint model based on a configuration with both SSE and speech-to-text models information with or without pre-trained checkpoints. After training, instead of saving two models separately, <code class="docutils literal notranslate"><span class="pre">enh_s2t_train.py</span></code> saves a joint-task checkpoint and configuration for the <code class="docutils literal notranslate"><span class="pre">enh_s2t</span></code> task.</p>
</section>
<section id="bin-asr-inference-py-bin-diar-inference-py-and-bin-st-inference-py">
<h4>bin.asr_inference.py, bin.diar_inference.py, and bin.st_inference.py<a class="headerlink" href="#bin-asr-inference-py-bin-diar-inference-py-and-bin-st-inference-py" title="Permalink to this headline">¶</a></h4>
<p>As the interface for the downstream speech-to-text decoding stage of the joint-task scripts, <code class="docutils literal notranslate"><span class="pre">asr_inference.py</span></code>, <code class="docutils literal notranslate"><span class="pre">st_inference.py</span></code> and <code class="docutils literal notranslate"><span class="pre">diar_inference.py</span></code> have</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Speech2Text</span>
<span class="k">class</span> <span class="nc">DiarizeSpeech</span>
</pre></div>
</div>
<p>to process speech for each task. During their initialization, the classes can build a back-end model object through their own back-end task classes or a joint-task object <code class="docutils literal notranslate"><span class="pre">ESPnetEnhS2TModel</span></code> through the <code class="docutils literal notranslate"><span class="pre">EnhS2TTask.build_model</span></code> class method with pre-trained joint-task models and configurations.</p>
<p>Calling the <code class="docutils literal notranslate"><span class="pre">SeparateSpeech</span></code> or <code class="docutils literal notranslate"><span class="pre">DiarizeSpeech</span></code> objects with an unprocessed audio returns a list of text results and tokens. The audio is enhanced and recognized by calling the <code class="docutils literal notranslate"><span class="pre">encode</span></code> function and <code class="docutils literal notranslate"><span class="pre">decode</span></code>or <code class="docutils literal notranslate"><span class="pre">beam_search</span></code> functions, where the <code class="docutils literal notranslate"><span class="pre">encode</span></code> function applies both the front-end SSE model and the encoder of the back-end model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">inference</span></code> function in <code class="docutils literal notranslate"><span class="pre">asr_inference.py</span></code>, <code class="docutils literal notranslate"><span class="pre">diar_inference.py</span></code>, and <code class="docutils literal notranslate"><span class="pre">st_inference.py</span></code> contains the full inference processes, which build and call a <code class="docutils literal notranslate"><span class="pre">SeparateSpeech</span></code> object or <code class="docutils literal notranslate"><span class="pre">DiarizeSpeech</span></code> object with the data-iterator for testing and validation.</p>
</section>
<section id="id1">
<h4>bin.enh_inference.py<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>As introduced in the SSE task section, <code class="docutils literal notranslate"><span class="pre">enh_inference.py</span></code> has <code class="docutils literal notranslate"><span class="pre">SeparateSpeech</span></code> object with an SSE object attribute <code class="docutils literal notranslate"><span class="pre">ESPnetEnhancementModel</span></code>. The <code class="docutils literal notranslate"><span class="pre">ESPnetEnhancementModel</span></code> object can be created through both <code class="docutils literal notranslate"><span class="pre">EnhancementTask.build_model</span></code> and <code class="docutils literal notranslate"><span class="pre">EnhS2TTask.build_model</span></code> class methods for the enhancing stage of the joint-task scripts.</p>
</section>
</section>
<section id="joint-task-control-class-tasks">
<h3>Joint-task Control Class <code class="docutils literal notranslate"><span class="pre">tasks/</span></code><a class="headerlink" href="#joint-task-control-class-tasks" title="Permalink to this headline">¶</a></h3>
<section id="tasks-enh-s2t-py">
<h4>tasks.enh_s2t.py<a class="headerlink" href="#tasks-enh-s2t-py" title="Permalink to this headline">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EnhS2TTask</span><span class="p">(</span><span class="n">AbsTask</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">EnhS2TTask</span></code> is designed for joint-task with an SSE model front-end and a back-end subtask model. Class method <code class="docutils literal notranslate"><span class="pre">build_model</span></code> creates a front-end and a back-end subtask models based on the <code class="docutils literal notranslate"><span class="pre">subtask_series</span></code> argument. The subtask models are then sent into the <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function from</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">enh</span><span class="o">.</span><span class="n">espnet_enh_s2t_model</span><span class="o">.</span><span class="n">ESPnetEnhS2TModel</span>
</pre></div>
</div>
<p>to create and return an joint-task object.</p>
</section>
</section>
<section id="joint-task-modules-enh">
<h3>Joint-Task Modules <code class="docutils literal notranslate"><span class="pre">enh/</span></code><a class="headerlink" href="#joint-task-modules-enh" title="Permalink to this headline">¶</a></h3>
<section id="enh-espnet-enh-s2t-model-py">
<h4>enh.espnet_enh_s2t_model.py<a class="headerlink" href="#enh-espnet-enh-s2t-model-py" title="Permalink to this headline">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ESPnetEnhS2TModel</span><span class="p">(</span><span class="n">AbsESPnetModel</span><span class="p">)</span>
</pre></div>
</div>
<p>Like <code class="docutils literal notranslate"><span class="pre">ESPnetEnhancementModel</span></code>, <code class="docutils literal notranslate"><span class="pre">ESPnetEnhS2TModel</span></code> inherits the abstract base class <code class="docutils literal notranslate"><span class="pre">AbsESPnetModel</span></code> and has the same interface as other tasks. The consistent modularized design enables the models in various tasks to be combined easily. In addition, the pre-trained checkpoints for different modules can be loaded into the joint model.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">__init__</span></code> function takes an <code class="docutils literal notranslate"><span class="pre">enh_model</span></code>, and an <code class="docutils literal notranslate"><span class="pre">s2t_model</span></code> as inputs to build a joint model containing both front-end and back-end models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">enh_model</span><span class="p">:</span> <span class="n">ESPnetEnhancementModel</span><span class="p">,</span>
    <span class="n">s2t_model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">ESPnetASRModel</span><span class="p">,</span> <span class="n">ESPnetSTModel</span><span class="p">,</span> <span class="n">ESPnetDiarizationModel</span><span class="p">],,</span> 
    <span class="o">...</span>
<span class="p">):</span>
</pre></div>
</div>
<p>The front-end model currently should be an SE model, and the back-end model could be ASR, SLU, ST, and SD models.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">forward</span></code> function of the class follows the general design in ESPnet2 single-task modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="k">def</span>  <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">speech_mix</span><span class="p">,</span> <span class="n">speech_ref</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<p>which processes speech and only returns losses for <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> to update the model.</p>
</section>
</section>
</section>
</section>
<section id="espnet-se-user-interface">
<h1>ESPnet-SE++ User Interface<a class="headerlink" href="#espnet-se-user-interface" title="Permalink to this headline">¶</a></h1>
<section id="building-a-new-recipe-from-scratch">
<h2>Building a New Recipe from Scratch<a class="headerlink" href="#building-a-new-recipe-from-scratch" title="Permalink to this headline">¶</a></h2>
<p>Since ESPnet2 provides common scripts such as <code class="docutils literal notranslate"><span class="pre">enh.sh</span></code> and <code class="docutils literal notranslate"><span class="pre">enh_asr.sh</span></code> for each task, given a new corpus ideally users would only need to create <code class="docutils literal notranslate"><span class="pre">local/data.sh</span></code> for the data preparation.</p>
<!-- ideally users would only need to create `local/data.sh` for the data preparation of the corpus. --><!-- adding a recipe for new corpus  , users can train models with their corpus without modifying the recipes. --><section id="prepare-the-data-local-data-sh">
<h3>Prepare the data : local/data.sh<a class="headerlink" href="#prepare-the-data-local-data-sh" title="Permalink to this headline">¶</a></h3>
<p>The data for each recipe is created by their own <code class="docutils literal notranslate"><span class="pre">local/data.sh</span></code>. The generated data follows the Kaldi-style structure:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>enh_asr1/
└── data/
    ├── train/
    │   ├── text      
    │   ├── spk1.scp
    │   ├── wav.scp  
    │   ├── utt2spk  
    │   ├── spk2utt  
    │   └── segments 
    ├── dev/
    └── test/
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">text</span></code>: The transcription for each utterance (Optional for the SSE task).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spk1.scp</span></code>: Wave file path to the clean utterances.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wav.scp</span></code>: Wave file path to the noisy utterances.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">utt2spk</span></code>: Mapping utterance-id to speaker-id.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">spk2utt</span></code>: Mapping speaker-id to utterance-id.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">segments</span></code>: Specifying the start and end time of each utterance (Optional).</p></li>
</ul>
<p>Since the tasks share the same data pre-processing format, the data can be easily used for different tasks. The detailed instructions for data preparation and building new recipes in espnet2 are described in the <a class="reference external" href="https://github.com/espnet/espnet/tree/master/egs2/TEMPLATE">egs2/TEMPLATE</a></p>
</section>
<section id="scoring-metrics-for-challenges-and-benchmarks-optional">
<h3>Scoring metrics for challenges and benchmarks (optional)<a class="headerlink" href="#scoring-metrics-for-challenges-and-benchmarks-optional" title="Permalink to this headline">¶</a></h3>
<p>Some of the challenges and datasets have their own metric calculation. The recipe-specific scoring scripts are prepared under the local/ directory of each recipe (if needed).</p>
<p>Example of the L3DAS22 Challenge:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Usage</span><span class="p">:</span> <span class="n">local</span><span class="o">/</span><span class="n">metric</span><span class="o">.</span><span class="n">sh</span> <span class="o">&lt;</span><span class="n">predicted</span> <span class="n">scp</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">target</span> <span class="n">scp</span><span class="o">&gt;</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">local</span><span class="o">/</span><span class="n">metric</span><span class="o">.</span><span class="n">sh</span> <span class="n">exp</span><span class="o">/</span><span class="n">enh_train_enh_ineube_raw</span><span class="o">/</span><span class="n">enhanced_test_multich</span><span class="o">/</span><span class="n">spk1</span><span class="o">.</span><span class="n">scp</span> <span class="n">dump</span><span class="o">/</span><span class="n">raw</span><span class="o">/</span><span class="n">test_multich</span><span class="o">/</span><span class="n">spk1</span><span class="o">.</span><span class="n">scp</span>

<span class="n">word</span> <span class="n">error</span> <span class="n">rate</span><span class="p">:</span> <span class="mf">4.85</span>
<span class="n">stoi</span><span class="p">:</span> <span class="mf">0.957</span>
<span class="n">task</span> <span class="mi">1</span> <span class="n">metric</span><span class="p">:</span> <span class="mf">0.954</span>
</pre></div>
</div>
</section>
</section>
<section id="inference-with-pre-trained-models">
<h2>Inference with Pre-trained Models<a class="headerlink" href="#inference-with-pre-trained-models" title="Permalink to this headline">¶</a></h2>
<p>Pretrained models from ESPnet are provided on HuggingFace and Zenodo. Users can download and infer with the models.<code class="docutils literal notranslate"><span class="pre">model_name</span></code> in the following section should be <code class="docutils literal notranslate"><span class="pre">huggingface_id</span></code> or one of the tags in the <a class="reference external" href="https://github.com/espnet/espnet_model_zoo/blob/master/espnet_model_zoo/table.csv">table.csv</a> in <a class="reference external" href="https://github.com/espnet/espnet_model_zoo">espnet_model_zoo</a> . User can also directly provide a Zenodo URL or a HuggingFace URL.</p>
<section id="inference-api">
<h3>Inference API<a class="headerlink" href="#inference-api" title="Permalink to this headline">¶</a></h3>
<p>The inference functions are from the <code class="docutils literal notranslate"><span class="pre">enh_inference</span></code> and <code class="docutils literal notranslate"><span class="pre">enh_asr_inference</span></code> in the executable code <code class="docutils literal notranslate"><span class="pre">bin/</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">espnet2.bin.enh_inference</span> <span class="kn">import</span> <span class="n">SeparateSpeech</span>
<span class="kn">from</span> <span class="nn">espnet2.bin.enh_asr_inference</span> <span class="kn">import</span> <span class="n">Speech2Text</span>
</pre></div>
</div>
<p>As described in the previous section, calling <code class="docutils literal notranslate"><span class="pre">SeparateSpeech</span></code> and <code class="docutils literal notranslate"><span class="pre">Speech2Text</span></code> with unprocessed audios returns the separated speech and their recognition results.</p>
<section id="sse">
<h4>SSE<a class="headerlink" href="#sse" title="Permalink to this headline">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">soundfile</span>
<span class="kn">from</span> <span class="nn">espnet2.bin.enh_inference</span> <span class="kn">import</span> <span class="n">SeparateSpeech</span>
<span class="n">separate_speech</span> <span class="o">=</span> <span class="n">SeparateSpeech</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;model_name&quot;</span><span class="p">,</span>
    <span class="c1"># load model from enh model or enh_s2t model</span>
    <span class="n">enh_s2t_task</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># for segment-wise process on long speech</span>
    <span class="n">segment_size</span><span class="o">=</span><span class="mf">2.4</span><span class="p">,</span>
    <span class="n">hop_size</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
    <span class="n">normalize_segment_scale</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">show_progressbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">ref_channel</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">normalize_output_wav</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># Confirm the sampling rate is equal to that of the training corpus.</span>
<span class="c1"># If not, you need to resample the audio data before inputting to speech2text</span>
<span class="n">speech</span><span class="p">,</span> <span class="n">rate</span> <span class="o">=</span> <span class="n">soundfile</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="s2">&quot;long_speech.wav&quot;</span><span class="p">)</span>
<span class="n">waves</span> <span class="o">=</span> <span class="n">separate_speech</span><span class="p">(</span><span class="n">speech</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">fs</span><span class="o">=</span><span class="n">rate</span><span class="p">)</span>
</pre></div>
</div>
<p>This API allows the processing of both short audio samples and long audio samples. For long audio samples, you can set the value of arguments <code class="docutils literal notranslate"><span class="pre">segment_size</span></code>, <code class="docutils literal notranslate"><span class="pre">hop_size</span></code> (optionally <code class="docutils literal notranslate"><span class="pre">normalize_segment_scale</span></code> and <code class="docutils literal notranslate"><span class="pre">show_progressbar</span></code>) to perform segment-wise speech enhancement/separation on the input speech. Note that the segment-wise processing is disabled by default.</p>
</section>
<section id="joint-task">
<h4>Joint-Task<a class="headerlink" href="#joint-task" title="Permalink to this headline">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">soundfile</span>
<span class="kn">from</span> <span class="nn">espnet2.bin.asr_inference</span> <span class="kn">import</span> <span class="n">Speech2Text</span>
<span class="n">speech2text</span> <span class="o">=</span> <span class="n">Speech2Text</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;model_name&quot;</span><span class="p">,</span>
    <span class="c1"># load model from enh_s2t model</span>
    <span class="n">enh_s2t_task</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># Decoding parameters are not included in the model file</span>
    <span class="n">maxlenratio</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">minlenratio</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">beam_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">ctc_weight</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">lm_weight</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="n">penalty</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="n">nbest</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
<span class="c1"># Confirm the sampling rate is equal to that of the training corpus.</span>
<span class="c1"># If not, you need to resample the audio data before inputting to speech2text</span>
<span class="n">speech</span><span class="p">,</span> <span class="n">rate</span> <span class="o">=</span> <span class="n">soundfile</span><span class="o">.</span><span class="n">read</span><span class="p">(</span><span class="s2">&quot;speech.wav&quot;</span><span class="p">)</span>
<span class="n">nbests</span><span class="p">,</span> <span class="n">waves</span> <span class="o">=</span> <span class="n">speech2text</span><span class="p">(</span><span class="n">speech</span><span class="p">)</span>

<span class="n">text</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">nbests</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>The details for downloading models and inference are described in the <a class="reference external" href="https://github.com/espnet/espnet_model_zoo">espnet_model_zoo</a>.</p>
</section>
</section>
</section>
</section>
<section id="demonstrations">
<h1>Demonstrations<a class="headerlink" href="#demonstrations" title="Permalink to this headline">¶</a></h1>
<p>The demonstrations of ESPnet-SE can be found in the following google colab links:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://colab.research.google.com/drive/1fjRJCh96SoYLZPRxsjF9VDv4Q2VoIckI?usp=sharing">ESPnet SSE Demonstration: CHiME-4 and WSJ0-2mix</a></p></li>
<li><p><a class="reference external" href="https://colab.research.google.com/drive/1hAR5hp8i0cBIMeku8LbGXseBBaF2gEyO#scrollTo=0kIjHfagi4T1">ESPnet-SE++ Joint-Task Demonstration: L3DAS22 Challenge and SLURP-Spatialized</a></p></li>
</ul>
</section>
<section id="development-plan">
<h1>Development plan<a class="headerlink" href="#development-plan" title="Permalink to this headline">¶</a></h1>
<p>The development plan of the ESPnet-SE++ can be found in <a class="reference external" href="https://github.com/espnet/espnet/issues/2200">Development plan for ESPnet2 speech enhancement</a>. In addition, the current joint-task design for the front-end task needs to be speech-to-speech transformation, and the back-end task needs to take speech as input to generate recognition, understanding, or translation results. In the future, we would like to extend to other type of combinations, such as using ASR as a front-end model and TTS as a back-end model to create a speech-to-speech conversion, making the selection of front-end and back-end modules more flexible.</p>
<!-- the TTS module is unavailable within the current structure.  --></section>
<section id="conclusions">
<h1>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">¶</a></h1>
<p>In this paper, we introduce the software structure and the user interface of ESPnet-SE++, including the SSE task and joint-task models. ESPnet-SE++ provides the general recipes for training models on different corpus and a simple way for adding new recipes. The joint-task implementation further shows that the modularized design improves the flexibility of ESPnet.</p>
</section>
<section id="acknowledgement">
<h1>Acknowledgement<a class="headerlink" href="#acknowledgement" title="Permalink to this headline">¶</a></h1>
<p>This work used the Extreme Science and Engineering Discovery Environment (XSEDE) [&#64;Towns:2014], which is supported by NSF grant number ACI-1548562. Specifically, it used the Bridges system [&#64;Nystrom:2015], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).</p>
</section>
<section id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2017, Shinji Watanabe.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>